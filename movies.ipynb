{
  "cells": [
    {
      "source": [
        "## Get list of movies from 2020\n",
        "This code has successfully retrieved a list of movies for 2020 and is no longer needed.  It is being archived for reference purposes.\n",
        "\n",
        "```python\n",
        "movies_2020_url = 'https://us.gowatching.com/mainpage/?utm_source=b10002_002_20200306'\n",
        "movies_2020 = requests.get(movies_2020_url)\n",
        "movies_2020_html = movies_2020.text\n",
        "soup_2020 = BeautifulSoup(movies_2020_html, 'lxml')\n",
        "soup_list_2020 = soup_2020.find_all(class_='movie_list')\n",
        "soup_list_2020 = soup_2020.find_all('a', class_='item owntitle')\n",
        "movie_titles_2020 = [title.get('title') for title in soup_list_2020]\n",
        "\n",
        "movie_file = open('2020_movies.txt', 'w')\n",
        "for movie in movie_titles_2020:\n",
        "    movie_file.write(movie)\n",
        "    movie_file.write('\\n')\n",
        "movie_file.close()\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Iterate over list of movies and get director, actor, rating, and budget data.\n",
        "\n",
        "```python\n",
        "# Only run this line once.\n",
        "movie_titles_2020 = pd.DataFrame(movie_titles_2020).T\n",
        "\n",
        "def get_movie_page(url, url_base='https://www.themoviedb.org'):\n",
        "    \"\"\"Get a movie's page from https://www.themoviedb.org.\n",
        "\n",
        "    Keyword arguments:\n",
        "    url_base -- the source website\n",
        "    url -- the path to the search page containing the movie\n",
        "    \"\"\"\n",
        "    query = '/search?query='\n",
        "    page = requests.get(url_base + query + url)\n",
        "    result_page = BeautifulSoup(page.text, 'lxml')\n",
        "    movie_url = url_base + result_page.find(class_ = 'result').get('href')\n",
        "\n",
        "    page = requests.get(movie_url)\n",
        "    return BeautifulSoup(page.text, 'lxml')\n",
        "\n",
        "def get_movie_details(movie_page):\n",
        "    \"\"\"Extract a movie's details from the source page\n",
        "\n",
        "    Keyword arguments:\n",
        "    movie_page -- the HTML of the movie's page\n",
        "    \"\"\"\n",
        "    rating = movie_page.find('span', class_ = 'certification').contents[0].strip()\n",
        "    director_node = movie_page.find('li', class_ = 'profile').find('a')\n",
        "    director = {'name': director_node.contents[0], 'url': director_node.get('href')}\n",
        "    actor_node = movie_page.find('ol', class_ = 'people scroller').find_all('a')\n",
        "    actors = [{'name': a.contents[0], 'url': a.get('href')} for i, a in enumerate(actor_node[:-1]) if i % 2 == 1]\n",
        "    budget = movie_page.find('section', class_ = 'facts left_column').find_all('p')[2].contents[1].strip()\n",
        "    genre_node = movie_page.find('span', class_ = 'genres').find_all('a')\n",
        "    genre = [{'type': node.contents[0], 'url': node.get('href')} for node in genre_node]\n",
        "    return [rating, director, actors, budget, genre]\n",
        "\n",
        "movie_list = []\n",
        "for title in movie_titles_2020[0]:\n",
        "    movie_page = get_movie_page(title)\n",
        "    movie_list.append(get_movie_details(movie_page))\n",
        "\n",
        "sys.setrecursionlimit(10000)\n",
        "pickle_file = open('movie_list.pkl', 'wb')\n",
        "pickle.dump(movie_list, pickle_file)\n",
        "pickle_file.close()\n",
        "sys.setrecursionlimit(3000)\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Get list of top grossing directors.\n",
        "\n",
        "```python\n",
        "top_directors = get_html('https://en.wikipedia.org/wiki/List_of_highest-grossing_directors')\n",
        "director_table = top_directors.find('table').find_all('a')\n",
        "director_list = [{'name': director.contents[0], 'url': director.get('href')} for i, director in enumerate(director_table) if i % 3 == 1]\n",
        "write_pickle(director_list, 'top_grossing_directors')\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Save list of movies released in 2020 as a pickle file instead of CSV.\n",
        "\n",
        "```python\n",
        "movie_titles_2020 = pd.DataFrame(movie_titles_2020).T\n",
        "write_pickle(movie_titles_2020, 'movie_titles_2020')\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Parses the director list and retrieves his or her filmography.\n",
        "\n",
        "```python\n",
        "director_list = [dire[1]['name'] for dire in movie_data]\n",
        "\n",
        "filmography = []\n",
        "for director_name in director_list:\n",
        "    director_page = search_tmdb(director_name)\n",
        "    if director_page != None:\n",
        "        filmography.append(\n",
        "            {'name': director_name, 'films': get_filmography_from_page(BeautifulSoup(director_page, 'lxml'))})\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Extracts movie data from table into a list\n",
        "```python\n",
        "bo_list = []\n",
        "for i in date_range:\n",
        "    page = requests.get('https://www.boxofficemojo.com/year/' +\n",
        "                        str(i) + '/?grossesOption=calendarGrosses')\n",
        "    bs_page = BeautifulSoup(page.text, 'lxml')\n",
        "    bs_table = bs_page.find_all('table')\n",
        "    bs_tr = bs_table[0].find_all('tr')[1:]\n",
        "    bo_list.append(bs_tr)\n",
        "\n",
        "data_list = []\n",
        "for row in bo_list[7]:\n",
        "    row_td = row.find_all('td')\n",
        "    one = row_td[1].find('a').get('href')\n",
        "    two = row_td[1].find('a').contents[0]\n",
        "    three = row_td[5].contents[0]\n",
        "    four = row_td[6].contents[0]\n",
        "    five = row_td[7].contents[0]\n",
        "    six = row_td[8].contents[0]\n",
        "    if row_td[9].find('a'):\n",
        "        seven = row_td[9].find('a').get('href')\n",
        "        eight = row_td[9].find('a').contents[0]\n",
        "    else:\n",
        "        seven = '-'\n",
        "        eight = '-'\n",
        "    data_list.append([one, two, three, four, five, six, seven, eight])\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Functions for scraping TheMovieDB.org\n",
        "\n",
        "```python\n",
        "def search_tmdb(search_string):\n",
        "    \"\"\"With the given search_string, sends a search request to TheMovieDB.org\n",
        "    and returns the search target's page as a BeautifulSoup object.\n",
        "\n",
        "    Keyword arguments:\n",
        "    search_string -- the name for which to search - escaping characters is not necessary.\n",
        "    \"\"\"\n",
        "    director_index = None\n",
        "    response_text = requests.get(\n",
        "        'https://www.themoviedb.org/search/person?query=' + search_string).text\n",
        "    response_soup = BeautifulSoup(response_text, 'lxml')\n",
        "    results = response_soup.find_all(class_='content')\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        if result.p:\n",
        "            if result.find_all('p')[1].span.contents[0] == 'Directing':\n",
        "                director_index = i\n",
        "                break\n",
        "\n",
        "    if director_index != None:\n",
        "        target_page = requests.get(\n",
        "            'https://www.themoviedb.org' + results[director_index].find(class_='result').get('href')).text\n",
        "    else:\n",
        "        target_page = None\n",
        "\n",
        "    return target_page\n",
        "\n",
        "\n",
        "def get_filmography_from_page(page):\n",
        "    \"\"\"Retrieves a director's filmography from TheMoviedb.org.\n",
        "\n",
        "    Keyword arguments:\n",
        "    page -- the HTML of the director's page\n",
        "    \"\"\"\n",
        "    directory = []\n",
        "    directing_index = -1\n",
        "\n",
        "    tabl = page.find_all('div', class_='credits_list')\n",
        "    h3 = tabl[0].find_all('h3')\n",
        "    for i, node in enumerate(h3):\n",
        "        if node.contents[0] == 'Directing':\n",
        "            directing_index = i\n",
        "            break\n",
        "    if len(h3):\n",
        "        directing = tabl[0].find_all('table', class_='card credits')[\n",
        "            directing_index].find_all('table')\n",
        "        for movie in directing:\n",
        "            year = movie.find('td', class_='year').contents[0]\n",
        "            if year.isnumeric() and int(year) < 2020:\n",
        "                aa = movie.find('a')\n",
        "                directory.append(\n",
        "                    {'name': aa.contents[0], 'url': aa.get('href')})\n",
        "\n",
        "    return directory\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Load and clean BoxOfficeMojo lists from 2010-2018 movies.\n",
        "\n",
        "```python\n",
        "movies_2018 = pd.DataFrame(read_csv('data/2018_movies'))\n",
        "movies_2018['datetime'] = pd.to_datetime(movies_2018[5] + ', 2018')\n",
        "movies_2017 = pd.DataFrame(read_csv('data/2017_movies'))\n",
        "movies_2017['datetime'] = pd.to_datetime(movies_2017[5] + ', 2017')\n",
        "movies_2016 = pd.DataFrame(read_csv('data/2016_movies'))\n",
        "movies_2016['datetime'] = pd.to_datetime(movies_2016[5] + ', 2016')\n",
        "movies_2015 = pd.DataFrame(read_csv('data/2015_movies'))\n",
        "movies_2015['datetime'] = pd.to_datetime(movies_2015[5] + ', 2015')\n",
        "movies_2014 = pd.DataFrame(read_csv('data/2014_movies'))\n",
        "movies_2014['datetime'] = pd.to_datetime(movies_2014[5] + ', 2014')\n",
        "movies_2013 = pd.DataFrame(read_csv('data/2013_movies'))\n",
        "movies_2013[5] = movies_2013[5].apply(\n",
        "    lambda x: 'Feb 28' if x == 'Feb 29' else x)\n",
        "movies_2013['datetime'] = pd.to_datetime(movies_2013[5] + ', 2013')\n",
        "movies_2012 = pd.DataFrame(read_csv('data/2012_movies'))\n",
        "movies_2012['datetime'] = pd.to_datetime(movies_2012[5] + ', 2012')\n",
        "movies_2011 = pd.DataFrame(read_csv('data/2011_movies'))\n",
        "movies_2011['datetime'] = pd.to_datetime(movies_2011[5] + ', 2011')\n",
        "movies_2010 = pd.DataFrame(read_csv('data/2010_movies'))\n",
        "movies_2010['datetime'] = pd.to_datetime(movies_2010[5] + ', 2010')\n",
        "\n",
        "all_movies = pd.concat((movies_2018, movies_2017, movies_2016, movies_2015, movies_2014,\n",
        "                        movies_2013, movies_2012, movies_2011, movies_2010))\n",
        "\n",
        "\n",
        "def CountFrequency(my_list):\n",
        "    freq = {}\n",
        "\n",
        "    for item in my_list:\n",
        "        if (item in freq):\n",
        "            freq[item] += 1\n",
        "        else:\n",
        "            freq[item] = 1\n",
        "\n",
        "    return freq\n",
        "\n",
        "\n",
        "# Filters distributors who have released more than 80 movies over the data set.\n",
        "distributor_count = CountFrequency(all_movies[7])\n",
        "\n",
        "# Manually select studios to reduce the number of featurs.\n",
        "# dists = list({k: v for (k, v) in distributor_count.items() if (v > 80) and (k != '-')})\n",
        "dists = ['Walt Disney Studios Motion Pictures', 'Universal Pictures', 'Twentieth Century Fox',\n",
        "         'Sony Pictures Entertainment (SPE)', 'Paramount Pictures', 'Warner Bros.']\n",
        "\n",
        "movie_set = all_movies.loc[all_movies[7].apply(\n",
        "    lambda x: x in dists), [2, 3, 5, 7]]\n",
        "movie_set.columns = ['Gross', 'Theaters', 'Date', 'Distributor']\n",
        "movie_set['Gross'] = movie_set['Gross'].apply(\n",
        "    lambda x: int(re.sub(r'[$,]', '', x)))\n",
        "movie_set['Theaters'] = movie_set['Theaters'].apply(\n",
        "    lambda x: x.replace('-', '0'))\n",
        "movie_set['Theaters'] = movie_set['Theaters'].apply(\n",
        "    lambda x: int(x.replace(',', '')))\n",
        "\n",
        "all_movies['Month'] = all_movies['datetime'].dt.month\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## From the complete list of movies, search IMDb by movie name and extract data.\n",
        "\n",
        "```python\n",
        "def search_imdb(name):\n",
        "    def sanitize(name):\n",
        "        clean = name.replace(' ', '+')\n",
        "        clean = clean.replace('&', '%26')\n",
        "\n",
        "        return clean\n",
        "\n",
        "    search_page = BeautifulSoup(requests.get(\n",
        "        'https://www.imdb.com/find?q=' + sanitize(name) + '&ref_=nv_sr_sm').text, 'lxml')\n",
        "    target = search_page.find_all('td', class_='result_text')\n",
        "    new_url = None\n",
        "    for td in target:\n",
        "        if td.a.contents[0] == name:\n",
        "            new_url = td.a.get('href')\n",
        "            break\n",
        "\n",
        "    if new_url:\n",
        "        time.sleep(.5+2*random.random())\n",
        "\n",
        "        target_page = BeautifulSoup(requests.get(\n",
        "            'https://www.imdb.com/' + new_url).text)\n",
        "        user_rating = target_page.find(class_='ratingValue').strong.span.contents[0] if target_page.find(\n",
        "            class_='ratingValue') else None\n",
        "        critic_rating = target_page.find(class_='metacriticScore').span.contents[0] if target_page.find(\n",
        "            class_='metacriticScore') else None\n",
        "        subtext = target_page.find('div', class_='subtext')\n",
        "        if subtext:\n",
        "            MPAA = subtext.contents[0].strip()\n",
        "            genre = [a.contents[0] for a in subtext.find_all('a')]\n",
        "        else:\n",
        "            MPAA = None\n",
        "            genre = None\n",
        "        director = target_page.find(class_='credit_summary_item').find(\n",
        "            'a') if target_page.find(class_='credit_summary_item') else None\n",
        "        budget = None\n",
        "        if target_page.find(id='titleDetails'):\n",
        "            details = target_page.find(id='titleDetails').find_all('div')\n",
        "            for h4 in details:\n",
        "                if h4.find('h4'):\n",
        "                    if h4.find('h4').contents[0] == 'Budget:':\n",
        "                        budget = h4.contents[2].strip()\n",
        "                        break\n",
        "\n",
        "        result = [name, user_rating, critic_rating,\n",
        "                  MPAA, genre, director, budget]\n",
        "    else:\n",
        "        result = [name, None, None, None, None, None, None]\n",
        "\n",
        "    return result\n",
        "\n",
        "movie_data = []\n",
        "for title in all_movies[1]:\n",
        "    print(title)\n",
        "    movie_data.append(search_imdb(title))\n",
        "    time.sleep(.5+2*random.random())\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## i/o functions\n",
        "```python\n",
        "def write_pickle(data, file_name):\n",
        "    \"\"\"Create a pickle file\n",
        "\n",
        "    Keyword arguments:\n",
        "    data -- the information to be saved\n",
        "    file_name -- the name of the file without an extension\n",
        "    \"\"\"\n",
        "    sys.setrecursionlimit(20000)\n",
        "    pickle_file = open(file_name + '.pkl', 'wb')\n",
        "    pickle.dump(data, pickle_file)\n",
        "    pickle_file.close()\n",
        "    sys.setrecursionlimit(3000)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def read_pickle(file_name):\n",
        "    \"\"\"Import a pickle file\n",
        "\n",
        "    Keyword arguments:\n",
        "    file_name -- the name of the file to be read without an extension\n",
        "    \"\"\"\n",
        "    pickle_file = open(file_name + '.pkl', 'rb')\n",
        "    file = pickle.load(pickle_file)\n",
        "    pickle_file.close()\n",
        "\n",
        "    return file\n",
        "\n",
        "\n",
        "def write_csv(data, file_name):\n",
        "    \"\"\"Saves a flat dictionary to a .CSV.\n",
        "\n",
        "    Keyword arguments:\n",
        "    data -- a flat dictionary\n",
        "    file_name -- the desired filename without an extension\n",
        "    \"\"\"\n",
        "    with open(file_name + '.csv', 'w', newline='') as csvfile:\n",
        "        csv_writer = csv.writer(csvfile, delimiter=',',\n",
        "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "        for row in data:\n",
        "            if type(row) == dict:\n",
        "                csv_writer.writerow(list(row.values()))\n",
        "            else:\n",
        "                csv_writer.writerow(row)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def read_csv(file_name):\n",
        "    \"\"\"Reads a .CSV into a list.\n",
        "\n",
        "    Keyword arguments:\n",
        "    file_name -- the desired file to load without an extension\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    with open(file_name + '.csv', newline='') as csvfile:\n",
        "        csv_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "        for row in csv_reader:\n",
        "            result.append(row)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_html(url):\n",
        "    \"\"\"Loads the HTML from the given URL.\n",
        "\n",
        "    Keyword arguments:\n",
        "    url -- the URL to load\n",
        "    \"\"\"\n",
        "    response_text = requests.get(url).text\n",
        "\n",
        "    return BeautifulSoup(response_text, 'lxml')\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "import time\n",
        "import csv\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import dateutil.parser\n",
        "from IPython.core.display import display, HTML\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# flatten filmography - used for director list, might use for actor list.\n",
        "flat_filmography = []\n",
        "\n",
        "for director in filmography:\n",
        "    for film in director['films']:\n",
        "        if film['url'].find('tv') == -1:\n",
        "            flat_filmography.append(\n",
        "                {'director': director['name'], 'film': film['name'].contents[0], 'url': film['url']})\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO get studio\n",
        "# TODO get number of studios in release\n",
        "# TODO get writer?\n",
        "# TODO get critic/user ratings?\n",
        "\n",
        "# TODO get actor filmographies\n",
        "# TODO get list of old releases from actor and director filmographies\n",
        "\n",
        "# TODO get ratings discard 'G'\n",
        "#      get directors - bucket by average movie rating\n",
        "#      get genres\n",
        "#      get budgets\n",
        "#      get more years\n",
        "#      adjust for inflation\n",
        "\n",
        "# Bucket directors, studios?, actors by earnings/salaries.\n",
        "# Genres are not exclusive."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "imdb_data = pd.read_csv('data/imdb_data.csv')\n",
        "all_movies = pd.read_csv('data/movies_from_2010_2018.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}